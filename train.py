import os
import glob
import gc
import torch
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr
from torch_geometric.loader import DataLoader
from model import FusionModel
from dataset import MultimodalBrainDataset
from utils import *
from config import *
from copy import deepcopy


def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for batch in loader:
        batch = batch.to(device)
        y = batch.y

        optimizer.zero_grad()
        preds = model(batch)
        loss = criterion(preds, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * len(batch)
    return total_loss / len(loader.dataset)

def evaluate(model, loader, criterion, device, scaler, region_labels=None, save_importance_csv=True, fold_idx=None):
    model.eval()
    total_loss = 0
    preds, labels = [], []
    all_func_importances = []
    all_struct_saliency_maps = []
    subject_ids = []

    for batch in loader:
        batch = batch.to(device)

        # ğŸ‘‡ è®¾ç½®ç»“æ„å›¾åƒéœ€è¦æ¢¯åº¦
        # batch.img = batch.img.clone().detach().requires_grad_(True)
        batch.img.requires_grad_(True)
        batch.img.retain_grad()

        # æ¸…é›¶å¯èƒ½å­˜åœ¨çš„æ—§æ¢¯åº¦
        if batch.img.grad is not None:
            batch.img.grad = None

        y = batch.y
        out = model(batch)
        loss = criterion(out, y)
        total_loss += loss.item() * len(batch)

        # ğŸ‘‡ å…³é”®æ­¥éª¤ï¼šå¯ç”¨åå‘ä¼ æ’­ä»¥ä¾¿è·å¾— saliency map
        # loss.backward(retain_graph=True)
        loss.backward()

        preds.extend(out.detach().cpu().numpy().flatten())
        labels.extend(y.detach().cpu().numpy().flatten())

        # è·å–åŠŸèƒ½å›¾çš„èŠ‚ç‚¹æ³¨æ„åŠ›ï¼ˆé‡è¦æ€§ï¼‰
        node_attention = model.gcn.get_node_importance()  # [total_nodes]

        for i in range(out.size(0)):
            node_mask = (batch.batch == i)
            importance = node_attention[node_mask].detach().cpu().numpy()  # [90]
            all_func_importances.append(importance)

            # è·å–ç»“æ„å›¾åƒçš„æ˜¾è‘—æ€§å›¾ï¼ˆsaliency mapï¼‰
            if batch.img.grad is not None:
                saliency_map = batch.img.grad[i].detach().cpu().numpy()
                saliency_map = np.abs(saliency_map).squeeze(0)
            else:
                saliency_map = np.zeros(batch.img[i].shape[1:], dtype=np.float32)
            all_struct_saliency_maps.append(saliency_map)

            cur_subject_id = str(batch.subject_id[i])
            subject_ids.append(cur_subject_id)

    # åå½’ä¸€åŒ–é¢„æµ‹ä¸æ ‡ç­¾
    preds = denormalize_ages(scaler, np.array(preds))
    labels = denormalize_ages(scaler, np.array(labels))

    mae = mean_absolute_error(labels, preds)
    rmse = np.sqrt(mean_squared_error(labels, preds))
    r2 = r2_score(labels, preds)
    pcc = pearsonr(labels, preds)[0]
    val_loss = total_loss / len(loader.dataset)

    all_func_importances = np.vstack(all_func_importances)

    # ä¿å­˜é‡è¦æ€§ä¸æ˜¾è‘—æ€§å›¾
    if save_importance_csv and region_labels is not None and fold_idx is not None:
        df_func = pd.DataFrame(all_func_importances, columns=region_labels)
        df_func.insert(0, 'subject_id', subject_ids)
        df_func.to_csv(f'func_oasis/{fold_idx+1}.csv', index=False)
        print(f"Fold {fold_idx+1} åŠŸèƒ½è„‘åŒºé‡è¦æ€§ä¿å­˜å®Œæˆï¼")

        for sid, sal_map in zip(subject_ids, all_struct_saliency_maps):
            np.save(f'saliency_map_oasis/fold{fold_idx + 1}_{sid}.npy', sal_map)
        print(f"Fold {fold_idx+1} ç»“æ„æ˜¾è‘—æ€§å›¾ä¿å­˜å®Œæˆï¼")

    return val_loss, mae, rmse, r2, pcc, labels, preds, all_func_importances


def run_train():
    # struct_paths = glob.glob(os.path.join(DATA_DIR_CAMCAN, '*', 'T1.nii.gz'))
    # subject_age_dict, scaler = load_age_info(CSV_PATH_CAMCAN)

    struct_paths = glob.glob(os.path.join(DATA_DIR_OASIS3, '*', 'brain.nii.gz'))  # æ³¨æ„è¿™é‡Œæ˜¯fMRIè·¯å¾„
    subject_age_dict, scaler = load_age_info(CSV_PATH_OASIS3)

    all_paths = combine_struct_fmri_paths(struct_paths)
    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)

    all_labels = []
    all_preds = []
    fold_train_losses = []
    fold_val_losses = []
    all_metrics = {'mae': [], 'rmse': [], 'r2': [], 'pcc': []}

    # ç”¨äºä¿å­˜æ¯ä¸ªfoldé¢„æµ‹ç»“æœ
    fold_results_list = []

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    for fold, (train_idx, val_idx) in enumerate(kf.split(all_paths)):
        print(f"\n===== Fold {fold + 1} =====")
        train_paths = [all_paths[i] for i in train_idx]
        val_paths = [all_paths[i] for i in val_idx]

        train_set = MultimodalBrainDataset(train_paths, subject_age_dict)
        val_set = MultimodalBrainDataset(val_paths, subject_age_dict)

        region_labels = val_set.get_region_labels()

        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=2, pin_memory=True)

        model = FusionModel().to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=LR)
        criterion = torch.nn.MSELoss()
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, min_lr=1e-6)

        best_mae = float('inf')
        best_model_state = None
        best_epoch = 0

        train_losses = []
        val_losses = []

        for epoch in range(EPOCHS):
            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
            val_loss, mae, rmse, r2, pcc, _, _, _ = evaluate(
                model, val_loader, criterion, device, scaler,
                region_labels=region_labels,
                save_importance_csv=False,
                fold_idx=fold
            )
            scheduler.step(val_loss)

            train_losses.append(train_loss)
            val_losses.append(val_loss)

            print(f"Epoch {epoch + 1}/{EPOCHS} | "
                  f"Train Loss: {train_loss:.4f} | "
                  f"Val Loss: {val_loss:.4f} | "
                  f"MAE: {mae:.4f} | R2: {r2:.4f}")

            if mae < best_mae:
                best_mae = mae
                best_model_state = deepcopy(model.state_dict())
                best_epoch = epoch + 1

            save_path = f"/home/zhuowan/code/Age_prediction/checkpoints_oasis/our/important_best_model_fold{fold + 1}.pt"
            torch.save(best_model_state, save_path)

        model.load_state_dict(best_model_state)

        # foldç»“æŸï¼ŒéªŒè¯æ—¶ä¿å­˜èŠ‚ç‚¹é‡è¦æ€§ï¼ˆæ‰€æœ‰éªŒè¯æ ·æœ¬ï¼‰
        val_loss, mae, rmse, r2, pcc, labels, preds, fold_importances = evaluate(
            model, val_loader, criterion, device, scaler,
            region_labels=region_labels,
            save_importance_csv=True,
            fold_idx=fold
        )

        fold_train_losses.append(train_losses)
        fold_val_losses.append(val_losses)

        # ä¿å­˜è¯¥foldçš„é¢„æµ‹ç»“æœDataFrame
        df_fold = pd.DataFrame({
            'fold': fold + 1,
            'actual_age': labels,
            'predicted_age': preds
        })
        fold_results_list.append(df_fold)

        all_labels.extend(labels)
        all_preds.extend(preds)

        all_metrics['mae'].append(mae)
        all_metrics['rmse'].append(rmse)
        all_metrics['r2'].append(r2)
        all_metrics['pcc'].append(pcc)

        print(f"âœ… Fold {fold + 1} æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Epoch {best_epoch}, MAE={best_mae:.4f})")

        del model, train_loader, val_loader
        gc.collect()
        torch.cuda.empty_cache()

    plot_scatter(all_labels, all_preds, fold_idx="all_folds", r=pearsonr(all_labels, all_preds)[0], phase="val")

    avg_train_loss = np.mean(fold_train_losses, axis=0)
    avg_val_loss = np.mean(fold_val_losses, axis=0)
    plot_loss_curve(avg_train_loss, avg_val_loss, fold_idx="avg")

    # ä¿å­˜æ‰€æœ‰foldé¢„æµ‹ç»“æœåˆ°CSV
    df_all = pd.concat(fold_results_list, ignore_index=True)
    df_all.to_csv("/home/zhuowan/code/Age_prediction/predictions_oasis/important_our_folds_predictions.csv", index=False)
    print("âœ… æ‰€æœ‰foldçš„é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° all_folds_predictions.csv")

    print("\n===== éªŒè¯é›†æœ€ç»ˆç»“æœ =====")
    print(f"å¹³å‡ MAE     : {np.mean(all_metrics['mae']):.4f} Â± {np.std(all_metrics['mae']):.4f}")
    print(f"å¹³å‡ RMSE    : {np.mean(all_metrics['rmse']):.4f} Â± {np.std(all_metrics['rmse']):.4f}")
    print(f"å¹³å‡ R2      : {np.mean(all_metrics['r2']):.4f} Â± {np.std(all_metrics['r2']):.4f}")
    print(f"å¹³å‡ PCC     : {np.mean(all_metrics['pcc']):.4f} Â± {np.std(all_metrics['pcc']):.4f}")


if __name__ == '__main__':
    torch.multiprocessing.set_start_method('spawn')
    run_train()
